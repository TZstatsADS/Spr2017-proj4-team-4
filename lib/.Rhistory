runlda <- lda.collapsed.gibbs.sampler(doc_format[[list_num]],k, vocab[[list_num]],
num.iterations = 1000, alpha = alpha, eta = beta)
# Calculate topic-word matrix
hw <- runlda$topics
W <- length(vocab[[list_num]])
sum_h <- rowSums(hw)
matrix_sumh <- matrix(rep(sum_h,k),nrow=k,ncol=W)
phi <- (hw+beta)/(matrix_sumh+W*beta)
# Calculate topic-document matrix
hd <- runlda$document_sums
D <- length(doc_format[[list_num]])
sum_hd <- colSums(hd)
matrix_sumhd <- matrix(rep(sum_hd,each=k),nrow=k,ncol=D)
theta <- (hd+alpha)/(matrix_sumhd+k*alpha)
colnames(theta) <- c(1:ncol(theta))
distance<- dist(data.frame(t(theta)))
# Agglomerative Clustering
clust.num<- max(gold_mat[[list_num]])
hcluster <- hclust(distance, "complete")
hclust_id <- cutree(hcluster, k=clust.num)
# showresult<- function(mat) {
#   result<- vector("list", clust.num)
#   for(i in 1:clust.num) {
#     result[[i]]<- names(mat)[mat == i]
#   }
#   return(result)
# }
# result<- showresult(hclust_id)
# compute error rate
match_mat<- matching_matrix(gold_mat[[list_num]], hclust_id)
perform<- performance_statistics(match_mat)
return(c(topicnumber=k, unlist(perform)))
}
main(1,5)
main(1,10)
sapply(5:7, main, list_num=1)
main <- function(list_num, topic_num) {
start.time <- Sys.time()
k<- topic_num
# parameter values
beta <- 0.01
alpha <- k/50
runlda <- lda.collapsed.gibbs.sampler(doc_format[[list_num]],k, vocab[[list_num]],
num.iterations = 1000, alpha = alpha, eta = beta)
# Calculate topic-word matrix
hw <- runlda$topics
W <- length(vocab[[list_num]])
sum_h <- rowSums(hw)
matrix_sumh <- matrix(rep(sum_h,k),nrow=k,ncol=W)
phi <- (hw+beta)/(matrix_sumh+W*beta)
# Calculate topic-document matrix
hd <- runlda$document_sums
D <- length(doc_format[[list_num]])
sum_hd <- colSums(hd)
matrix_sumhd <- matrix(rep(sum_hd,each=k),nrow=k,ncol=D)
theta <- (hd+alpha)/(matrix_sumhd+k*alpha)
colnames(theta) <- c(1:ncol(theta))
distance<- dist(data.frame(t(theta)))
# Agglomerative Clustering
clust.num<- max(gold_mat[[list_num]])
hcluster <- hclust(distance, "complete")
hclust_id <- cutree(hcluster, k=clust.num)
# showresult<- function(mat) {
#   result<- vector("list", clust.num)
#   for(i in 1:clust.num) {
#     result[[i]]<- names(mat)[mat == i]
#   }
#   return(result)
# }
# result<- showresult(hclust_id)
# compute error rate
match_mat<- matching_matrix(gold_mat[[list_num]], hclust_id)
perform<- performance_statistics(match_mat)
end.time <- Sys.time()
return(c(unlist(perform), cluster.time = end.time- start.time))
}
tune <- function(list_num, min_k, max_k) {
start.time <- Sys.time()
k_vec<- c(min_k, max_k)
perform_mat <- sapply(k_vec ,main, list_num = list_num)
acc_vec <- perform_vec[4,]
best_ind <- which.min(acc_vec)
best_k <- k_vec[best_ind]
end.time <- Sys.time()
return(c(best.k = best_k, perform_mat[best_ind,],
tune.time = end.time - start.time))
}
tune(1,5,10)
tune <- function(list_num, min_k, max_k) {
start.time <- Sys.time()
k_vec<- c(min_k, max_k)
perform_mat <- sapply(k_vec ,main, list_num = list_num)
acc_vec <- perform_mat[4,]
best_ind <- which.min(acc_vec)
best_k <- k_vec[best_ind]
end.time <- Sys.time()
return(c(best.k = best_k, perform_mat[best_ind,],
tune.time = end.time - start.time))
}
tune(1,5,10)
sapply(5:10, main, list_num=1)
tune <- function(list_num, min_k, max_k) {
start.time <- Sys.time()
k_vec<- c(min_k, max_k)
perform_mat <- sapply(k_vec ,main, list_num = list_num)
acc_vec <- perform_mat[4,]
best_ind <- which.max(acc_vec)
best_k <- k_vec[best_ind]
end.time <- Sys.time()
return(c(best.k = best_k, perform_mat[,best_ind],
tune.time = end.time - start.time))
}
tune(1,5,10)
tune(2,5,10)
tune(3,5,10)
data_list[[3]]
tune(2,5,10)
tune(4,5,10)
tune(3,5,10)
final_result<- sapply(1:14, tune, min_k=5, max_k=10)
final_result
tune <- function(list_num) {
start.time <- Sys.time()
k_vec<- c(5, 10)
perform_mat <- sapply(k_vec ,main, list_num = list_num)
acc_vec <- perform_mat[4,]
best_ind <- which.max(acc_vec)
best_k <- k_vec[best_ind]
end.time <- Sys.time()
return(c(best.k = best_k, perform_mat[,best_ind],
tune.time = end.time - start.time))
}
final_result<- sapply(1:14, tune, min_k=5, max_k=10)
final_result<- sapply(1:14, tune0)
final_result<- sapply(1:14, tune)
rownames(final_result)<- query.list
colnames(final_result)<- query.list
final_result
end.time0 - start.time0
list_num=1
topic_num=5
k<- topic_num
beta <- 0.01
alpha <- k/50
runlda <- lda.collapsed.gibbs.sampler(doc_format[[list_num]],
k, vocab[[list_num]],
num.iterations = 1000,
alpha = alpha, eta = beta)
hw <- runlda$topics
W <- length(vocab[[list_num]])
sum_h <- rowSums(hw)
matrix_sumh <- matrix(rep(sum_h,k),nrow=k,ncol=W)
phi <- (hw+beta)/(matrix_sumh+W*beta)
hd <- runlda$document_sums
D <- length(doc_format[[list_num]])
sum_hd <- colSums(hd)
matrix_sumhd <- matrix(rep(sum_hd,each=k),nrow=k,ncol=D)
theta <- (hd+alpha)/(matrix_sumhd+k*alpha)
colnames(theta) <- c(1:ncol(theta))
distance<- dist(data.frame(t(theta)))
k=10
runlda <- lda.collapsed.gibbs.sampler(doc_format[[list_num]],
k, vocab[[list_num]],
num.iterations = 1000,
alpha = alpha, eta = beta)
hw <- runlda$topics
W <- length(vocab[[list_num]])
sum_h <- rowSums(hw)
matrix_sumh <- matrix(rep(sum_h,k),nrow=k,ncol=W)
phi <- (hw+beta)/(matrix_sumh+W*beta)
hd <- runlda$document_sums
D <- length(doc_format[[list_num]])
sum_hd <- colSums(hd)
matrix_sumhd <- matrix(rep(sum_hd,each=k),nrow=k,ncol=D)
theta <- (hd+alpha)/(matrix_sumhd+k*alpha)
colnames(theta) <- c(1:ncol(theta))
start.time0 <- Sys.time()
if (!require("lda")) install.packages("lda")
library(lda)
source("../lib/cleandata.R")
source("../lib/evaluation_measures.R")
# set up the corpus of each document
corpus<- function(llist){
document<- function(lllist) {
words_apperance <- c(unlist(strsplit(lllist[[4]], " ")), unlist(strsplit(lllist[[5]], " ")))
name_appearance <- unlist(lllist[[3]])
doc <- list(words = words_apperance, name = name_appearance)
return(doc)
}
return(lapply(llist, document))
}
doc_corpus<- lapply(data_list, corpus)
# extract all words(coauthor names, title of the paper and published journal of each documentation)
allwords_fun<- function(llist) {
allwords_fun1<- function(lllist) {
return(c(lllist$name, lllist$words))
}
return(lapply(llist, allwords_fun1))
}
doc_allwords<- lapply(doc_corpus, allwords_fun)
# the vocab(all words) used in the documents of each name)
vocab_fun<- function(llist) {
vocab<- c()
n<- length(llist)
for(i in 1:n){
vocab<- c(vocab,llist[[i]])
}
vocab<- unique(vocab)
return(vocab)
}
vocab<- lapply(doc_allwords, vocab_fun)
# extract the Gold standard clusters for each author name
query.g<- function(llist){
gold<- function(lllist) {
gold_id<- lllist[[1]]
return(gold_id)
}
return(sapply(llist, gold))
}
gold_mat<- sapply(data_list, query.g)
# construct the list format we should use in the code
index<- function(x,a) {return(which(a==x))}
doc_format<- vector("list", 14)
for(i in 1:14) {
format_fun<- function(lllist) {
t<- table(lllist)
vec1<- as.numeric(sapply(names(t), index, a=vocab[[i]]))-1
vec2<- as.numeric(t)
m<- matrix(as.integer(c(vec1, vec2)), ncol = 2)
return(t(m))
}
doc_format[[i]] <- lapply(doc_allwords[[i]], format_fun)
}
names(doc_format)<- query.list
# main function to run LDA model and tune the parameter
# Input: list_num: the index of the each list, for example: list_num = 1 represents the author name is "A Gupta"
#        topic_num: the parameter used in LDA
main <- function(list_num, topic_num) {
start.time <- Sys.time()
k<- topic_num
# parameter values
beta <- 0.01
alpha <- k/50
# run the LDA
runlda <- lda.collapsed.gibbs.sampler(doc_format[[list_num]],
k, vocab[[list_num]],
num.iterations = 1000,
alpha = alpha, eta = beta)
# Calculate topic-word matrix
hw <- runlda$topics
W <- length(vocab[[list_num]])
sum_h <- rowSums(hw)
matrix_sumh <- matrix(rep(sum_h,k),nrow=k,ncol=W)
phi <- (hw+beta)/(matrix_sumh+W*beta)
# Calculate topic-document probability matrix
hd <- runlda$document_sums
D <- length(doc_format[[list_num]])
sum_hd <- colSums(hd)
matrix_sumhd <- matrix(rep(sum_hd,each=k),nrow=k,ncol=D)
theta <- (hd+alpha)/(matrix_sumhd+k*alpha)
colnames(theta) <- c(1:ncol(theta))
distance<- dist(data.frame(t(theta)))
# Agglomerative Clustering
clust.num<- max(gold_mat[[list_num]])
hcluster <- hclust(distance, "complete")
# compute accuracy based on precision anf recall
match_mat<- matching_matrix(gold_mat[[list_num]], hclust_id)
perform<- performance_statistics(match_mat)
end.time <- Sys.time()
return(c(unlist(perform), cluster.time = end.time- start.time))
}
# tune the parameter(topic number) to get the max accuracy
# the best topic number is either 5 or 10 based on the paper
tune <- function(list_num) {
start.time <- Sys.time()
k_vec<- c(5, 10)
perform_mat <- sapply(k_vec ,main, list_num = list_num)
acc_vec <- perform_mat[4,]
best_ind <- which.max(acc_vec)
best_k <- k_vec[best_ind]
end.time <- Sys.time()
return(c(best.k = best_k, perform_mat[,best_ind],
tune.time = end.time - start.time))
}
tune(1)
gold_mat[[list_num]]
list_num=1
gold_mat[[list_num]]
max(gold_mat[[list_num]])
# main function to run LDA model and tune the parameter
# Input: list_num: the index of the each list, for example: list_num = 1 represents the author name is "A Gupta"
#        topic_num: the parameter used in LDA
main <- function(list_num, topic_num) {
start.time <- Sys.time()
k<- topic_num
# parameter values
beta <- 0.01
alpha <- k/50
# run the LDA
runlda <- lda.collapsed.gibbs.sampler(doc_format[[list_num]],
k, vocab[[list_num]],
num.iterations = 1000,
alpha = alpha, eta = beta)
# Calculate topic-word matrix
hw <- runlda$topics
W <- length(vocab[[list_num]])
sum_h <- rowSums(hw)
matrix_sumh <- matrix(rep(sum_h,k),nrow=k,ncol=W)
phi <- (hw+beta)/(matrix_sumh+W*beta)
# Calculate topic-document probability matrix
hd <- runlda$document_sums
D <- length(doc_format[[list_num]])
sum_hd <- colSums(hd)
matrix_sumhd <- matrix(rep(sum_hd,each=k),nrow=k,ncol=D)
theta <- (hd+alpha)/(matrix_sumhd+k*alpha)
colnames(theta) <- c(1:ncol(theta))
distance<- dist(data.frame(t(theta)))
# Agglomerative Clustering
clust.num<- max(gold_mat[[list_num]])
hcluster <- hclust(distance, "complete")
hclust_id<- cutree(hcluster, gold_mat[[list_num]])
# compute accuracy based on precision anf recall
match_mat<- matching_matrix(gold_mat[[list_num]], hclust_id)
perform<- performance_statistics(match_mat)
end.time <- Sys.time()
return(c(unlist(perform), cluster.time = end.time- start.time))
}
# tune the parameter(topic number) to get the max accuracy
# the best topic number is either 5 or 10 based on the paper
tune <- function(list_num) {
start.time <- Sys.time()
k_vec<- c(5, 10)
perform_mat <- sapply(k_vec ,main, list_num = list_num)
acc_vec <- perform_mat[4,]
best_ind <- which.max(acc_vec)
best_k <- k_vec[best_ind]
end.time <- Sys.time()
return(c(best.k = best_k, perform_mat[,best_ind],
tune.time = end.time - start.time))
}
tune(1)
tune(2)
tune(3)
# main function to run LDA model and tune the parameter
# Input: list_num: the index of the each list, for example: list_num = 1 represents the author name is "A Gupta"
#        topic_num: the parameter used in LDA
main <- function(list_num, topic_num) {
start.time <- Sys.time()
k<- topic_num
# parameter values
beta <- 0.01
alpha <- k/50
# run the LDA
runlda <- lda.collapsed.gibbs.sampler(doc_format[[list_num]],
k, vocab[[list_num]],
num.iterations = 1000,
alpha = alpha, eta = beta)
# Calculate topic-word matrix
hw <- runlda$topics
W <- length(vocab[[list_num]])
sum_h <- rowSums(hw)
matrix_sumh <- matrix(rep(sum_h,k),nrow=k,ncol=W)
phi <- (hw+beta)/(matrix_sumh+W*beta)
# Calculate topic-document probability matrix
hd <- runlda$document_sums
D <- length(doc_format[[list_num]])
sum_hd <- colSums(hd)
matrix_sumhd <- matrix(rep(sum_hd,each=k),nrow=k,ncol=D)
theta <- (hd+alpha)/(matrix_sumhd+k*alpha)
colnames(theta) <- c(1:ncol(theta))
distance<- dist(data.frame(t(theta)))
# Agglomerative Clustering
clust.num<- max(gold_mat[[list_num]])
hcluster <- hclust(distance, "complete")
hclust_id<- cutree(hcluster, gold_mat[[list_num]])
# compute accuracy based on precision anf recall
match_mat<- matching_matrix(gold_mat[[list_num]], hclust_id)
perform<- performance_statistics(match_mat)
end.time <- Sys.time()
return(c(unlist(perform), cluster.time = end.time- start.time))
}
# tune the parameter(topic number) to get the max accuracy
# the best topic number is either 5 or 10 based on the paper
tune <- function(list_num) {
start.time <- Sys.time()
k_vec<- c(5, 10)
perform_mat <- sapply(k_vec ,main, list_num = list_num)
acc_vec <- perform_mat[4,]
best_ind <- which.max(acc_vec)
best_k <- k_vec[best_ind]
end.time <- Sys.time()
return(c(best.k = best_k, perform_mat[,best_ind],
tune.time = end.time - start.time))
}
# final results
final_result<- sapply(1:14, tune)
start.time0 <- Sys.time()
if (!require("lda")) install.packages("lda")
library(lda)
source("../lib/cleandata.R")
source("../lib/evaluation_measures.R")
# set up the corpus of each document
corpus<- function(llist){
document<- function(lllist) {
words_apperance <- c(unlist(strsplit(lllist[[4]], " ")), unlist(strsplit(lllist[[5]], " ")))
name_appearance <- unlist(lllist[[3]])
doc <- list(words = words_apperance, name = name_appearance)
return(doc)
}
return(lapply(llist, document))
}
doc_corpus<- lapply(data_list, corpus)
# extract all words(coauthor names, title of the paper and published journal of each documentation)
allwords_fun<- function(llist) {
allwords_fun1<- function(lllist) {
return(c(lllist$name, lllist$words))
}
return(lapply(llist, allwords_fun1))
}
doc_allwords<- lapply(doc_corpus, allwords_fun)
# the vocab(all words) used in the documents of each name)
vocab_fun<- function(llist) {
vocab<- c()
n<- length(llist)
for(i in 1:n){
vocab<- c(vocab,llist[[i]])
}
vocab<- unique(vocab)
return(vocab)
}
vocab<- lapply(doc_allwords, vocab_fun)
# extract the Gold standard clusters for each author name
query.g<- function(llist){
gold<- function(lllist) {
gold_id<- lllist[[1]]
return(gold_id)
}
return(sapply(llist, gold))
}
gold_mat<- sapply(data_list, query.g)
# construct the list format we should use in the code
index<- function(x,a) {return(which(a==x))}
doc_format<- vector("list", 14)
for(i in 1:14) {
format_fun<- function(lllist) {
t<- table(lllist)
vec1<- as.numeric(sapply(names(t), index, a=vocab[[i]]))-1
vec2<- as.numeric(t)
m<- matrix(as.integer(c(vec1, vec2)), ncol = 2)
return(t(m))
}
doc_format[[i]] <- lapply(doc_allwords[[i]], format_fun)
}
names(doc_format)<- query.list
# main function to run LDA model and tune the parameter
# Input: list_num: the index of the each list, for example: list_num = 1 represents the author name is "A Gupta"
#        topic_num: the parameter used in LDA
main <- function(list_num, topic_num) {
start.time <- Sys.time()
k<- topic_num
# parameter values
beta <- 0.01
alpha <- k/50
# run the LDA
runlda <- lda.collapsed.gibbs.sampler(doc_format[[list_num]],
k, vocab[[list_num]],
num.iterations = 1000,
alpha = alpha, eta = beta)
# Calculate topic-word matrix
hw <- runlda$topics
W <- length(vocab[[list_num]])
sum_h <- rowSums(hw)
matrix_sumh <- matrix(rep(sum_h,k),nrow=k,ncol=W)
phi <- (hw+beta)/(matrix_sumh+W*beta)
# Calculate topic-document probability matrix
hd <- runlda$document_sums
D <- length(doc_format[[list_num]])
sum_hd <- colSums(hd)
matrix_sumhd <- matrix(rep(sum_hd,each=k),nrow=k,ncol=D)
theta <- (hd+alpha)/(matrix_sumhd+k*alpha)
colnames(theta) <- c(1:ncol(theta))
distance<- dist(data.frame(t(theta)))
# Agglomerative Clustering
clust.num<- max(gold_mat[[list_num]])
hcluster <- hclust(distance, "complete")
hclust_id<- cutree(hcluster, gold_mat[[list_num]])
# compute accuracy based on precision anf recall
match_mat<- matching_matrix(gold_mat[[list_num]], hclust_id)
perform<- performance_statistics(match_mat)
end.time <- Sys.time()
return(c(unlist(perform), cluster.time = end.time- start.time))
}
# tune the parameter(topic number) to get the max accuracy
# the best topic number is either 5 or 10 based on the paper
tune <- function(list_num) {
start.time <- Sys.time()
k_vec<- c(5, 10)
perform_mat <- sapply(k_vec ,main, list_num = list_num)
acc_vec <- perform_mat[4,]
best_ind <- which.max(acc_vec)
best_k <- k_vec[best_ind]
end.time <- Sys.time()
return(c(best.k = best_k, perform_mat[,best_ind],
tune.time = end.time - start.time))
}
# final results
final_result<- sapply(1:14, tune)
colnames(final_result)<- query.list
final_result
end.time0<- Sys.time()
# the whole time to complete our algorithm on Name Disambiguation
end.time0 - start.time0
